# ML Products Returns

Une entreprise de e-commerce souhaite pr√©dire la probabilit√© de retour d‚Äôun produit apr√®s un achat, afin d‚Äôoptimiser ses recommandations et ses stocks. L‚Äôobjectif est d‚Äôindustrialiser ce mod√®le de machine learning dans un pipeline de traitement distribu√© (PySpark), avec un focus sur la testabilit√© et la maintenabilit√© gr√¢ce √† une architecture hexagonale.

## Architecture cible : Hexagonale
### Couches d√©finies :

* Domaine : logique m√©tier ind√©pendante (entit√©s, services de pr√©diction, validation).
* Application : orchestration, cas d‚Äôusage (entra√Ænement, pr√©diction, scoring).
* Infrastructure : persistance (HDFS, S3, DB), lecture/√©criture de fichiers, interaction avec Spark, mod√®les s√©rialis√©s, etc.
* Ports (interfaces) : abstractions vers l‚Äôint√©rieur (use cases) ou l‚Äôext√©rieur (adaptateurs).

### Workflow global

1. Ingestion des donn√©es depuis un data lake (S3 ou HDFS).
2. Pr√©-traitement et feature engineering (PySpark).
3. Entra√Ænement du mod√®le (scikit-learn).
4. S√©rialisation du mod√®le (MLflow, Pickle ou Joblib).
5. API ou Batch de pr√©diction (Spark ou REST Flask/FastAPI).
6. Monitoring / Logging (MLflow + logs PySpark).
7. Tests automatis√©s (Pytest avec mocks sur l‚Äôinfrastructure).

##  Stack technique

| Besoin                   | Outil(s)                          |
|--------------------------|-----------------------------------|
| Traitement distribu√©     | PySpark                           |
| Mod√©lisation             | scikit-learn                      |
| S√©rialisation du mod√®le  | MLflow / Joblib                   |
| Orchestration            | Luigi                             |
| Architecture             | Clean / Hexagonale                |
| Tests                    | pytest + unittest.mock            |
| CI/CD                    | GitHub Actions                    |
| Packaging                | Poetry                            |
| Lint                     | Ruff                              |
| Typage                   | mypy                              |
| Logs                     | loguru                            |

## Installation/Execution via Docker

1. Cr√©er des credentials sur Kaggle, et obtenir des cl√©s API.
2. Cr√©er le fichier `.env` √† partir du fichier `.env.dist`

Enfin :

```
make build
make up
```
### Apps

* Minio => http://localhost:9001/
* MLflow => http://localhost:5000/
* Luigi (√† activer) => http://localhost:8082/


## Installation via Windows (üò≠üò≠)

### Pr√©requis syst√®me
#### Python & gestion de projet

* Python 3.12
* Poetry pour g√©rer les d√©pendances

#### Java

* Java 21 recommand√©

>‚ö†Ô∏è Java 24 non support√© par Hadoop/Spark ‚Üí UnsupportedOperationException: getSubject)

Ajoute JAVA_HOME dans ton environnement :

```
$env:JAVA_HOME = "C:\Program Files\Java\jdk-21"
$env:Path = "$env:JAVA_HOME\bin;$env:Path"
```

### Minio

```
$env:MINIO_ENDPOINT="http://localhost:9000"
$env:MINIO_ACCESS_KEY="minio"
$env:MINIO_SECRET_KEY="minio123"
```

### Kaggle Hub

```
$env:KAGGLE_USERNAME="mickaelandrieu"
$env:KAGGLE_KEY="d929b44284f3ea32a994136a87fcd365"
```

### Luigi (configuration de la Target Minio)

```
$env:AWS_ACCESS_KEY_ID="minio"
$env:AWS_SECRET_ACCESS_KEY="minio123"
$env:LUIGI_S3_ENDPOINT="http://localhost:9000"
```

### MLFlow

```
$env:MLFLOW_S3_ENDPOINT_URL="http://localhost:9000"
$env:AWS_ACCESS_KEY_ID="minio"
$env:AWS_SECRET_ACCESS_KEY="minio123"
```

### Sp√©cifique Java/Spark sur Windows

```
$env:PYSPARK_SUBMIT_ARGS="--conf spark.driver.extraJavaOptions=--add-opens=java.base/javax.security.auth=ALL-UNNAMED pyspark-shell"
$env:JAVA_TOOL_OPTIONS="--enable-native-access=ALL-UNNAMED"
```



### Apache Spark & Hadoop
#### Spark

* Spark 3.5.5 (pr√©compil√© avec Hadoop 3)

```
$env:SPARK_HOME = "C:\spark"
$env:PYSPARK_SUBMIT_ARGS="--conf spark.driver.extraJavaOptions=--add-opens=java.base/javax.security.auth=ALL-UNNAMED pyspark-shell"
```

(Windows ‚Üí zip √† extraire dans C:\spark\...)

#### Hadoop

```
$env:HADOOP_HOME = "C:\spark"
$env:hadoop_home_dir = "C:\spark"
```

### MinIO (S3 local)
Lancer MinIO dans Docker :

```
docker run -p 9000:9000 -p 9001:9001 \
  -e MINIO_ACCESS_KEY=minio \
  -e MINIO_SECRET_KEY=minio123 \
  quay.io/minio/minio server /data --console-address ":9001"
```

Acc√®s web : http://localhost:9001

Buckets √† cr√©er (automatiquement si besoin) :

* datasets
* models
* luigi-checkpoints

### üì¶ JARs Hadoop √† ajouter manuellement (dans spark/jars/)
Pour que Spark puisse lire/√©crire sur MinIO via S3A :

* hadoop-aws-3.3.4.jar
* aws-java-sdk-bundle-1.11.1026.jar

A placer dans le dossier `C:\spark\jars\`